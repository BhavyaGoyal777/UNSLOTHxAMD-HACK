{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-Agent Training - Llama-3.2-3B with GRPO\n",
    "\n",
    "Model: Llama-3.2-3B-Instruct (3 billion parameters)\n",
    "\n",
    "Task: Answer logical reasoning questions\n",
    "\n",
    "Training: SFT + GRPO (reward for correct answers and proper reasoning)\n",
    "\n",
    "NO emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt\n",
    "from trl import SFTConfig, SFTTrainer, GRPOConfig, GRPOTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "print(\"Imports successful\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURATED_DIR = Path(\"MAIN_CURATED_JSON\")\n",
    "MODELS_DIR = Path(\"FINAL_MODELS\")\n",
    "\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Directories ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all curated questions\n",
    "all_questions = []\n",
    "curated_files = sorted(CURATED_DIR.glob(\"*.json\"))\n",
    "\n",
    "for file_path in curated_files:\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            questions = json.load(f)\n",
    "            for q in questions:\n",
    "                if len(q.get('choices', [])) == 4 and q.get('answer', '') in ['A', 'B', 'C', 'D']:\n",
    "                    all_questions.append(q)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path.name}: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(all_questions)} valid questions\")\n",
    "print(f\"\\nSample:\")\n",
    "print(json.dumps(all_questions[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_AGENT_SYSTEM_PROMPT = \"\"\"You are A-Agent, an expert logical reasoning solver for the AMD AI Dev Day Hackathon.\n",
    "\n",
    "Your task is to analyze multiple choice questions on blood relations and seating arrangements, then provide the correct answer with detailed step-by-step reasoning.\n",
    "\n",
    "BLOOD RELATIONS SOLVING STRATEGY:\n",
    "\n",
    "1. IDENTIFY THE SPEAKER PERSPECTIVE:\n",
    "   - Determine whose perspective the question is from\n",
    "   - \\\"my\\\" always refers to the speaker\n",
    "   - Fix the reference point before analyzing relationships\n",
    "\n",
    "2. RESOLVE EACH RELATION STEP-BY-STEP:\n",
    "   - \\\"my grandfather's only son\\\" = my father\n",
    "   - \\\"my mother's daughter\\\" = my sister (or me if female)\n",
    "   - \\\"my father's father\\\" = my grandfather\n",
    "   - \\\"my uncle's wife\\\" = my aunt\n",
    "   - Break down complex chains into simple parent-child or sibling links\n",
    "\n",
    "3. MAINTAIN GENERATION LEVELS:\n",
    "   Track the generational hierarchy:\n",
    "   - Grandparent level: -2 (grandfather, grandmother)\n",
    "   - Parent level: -1 (father, mother, uncle, aunt)\n",
    "   - Self level: 0 (brother, sister, cousin)\n",
    "   - Child level: +1 (son, daughter, nephew, niece)\n",
    "   - Grandchild level: +2 (grandson, granddaughter)\n",
    "   Never confuse upward relations (toward ancestors) with downward relations (toward descendants)\n",
    "\n",
    "4. GENDER ASSUMPTIONS:\n",
    "   - If gender is not explicitly stated, assume the speaker is male\n",
    "   - Pay attention to gender-specific terms: he/she, his/her, husband/wife, brother/sister\n",
    "\n",
    "5. FINALIZE THE RELATION:\n",
    "   - After mapping all relationships, determine the final connection\n",
    "   - Use standard family terms: father, mother, son, daughter, brother, sister, uncle, aunt, cousin, nephew, niece, grandfather, grandmother, brother-in-law, sister-in-law\n",
    "   - Select the choice that matches this relationship\n",
    "\n",
    "SEATING ARRANGEMENT SOLVING STRATEGY:\n",
    "\n",
    "1. IDENTIFY THE ARRANGEMENT TYPE:\n",
    "   - Linear: People sit in a straight row (left to right or numbered positions)\n",
    "   - Circular: People sit around a table (clockwise/anticlockwise direction matters)\n",
    "\n",
    "2. DETERMINE FACING DIRECTION (for circular only):\n",
    "   - Facing center: \\\"left\\\" means clockwise direction, \\\"right\\\" means anticlockwise\n",
    "   - Facing outward: \\\"left\\\" means anticlockwise direction, \\\"right\\\" means clockwise\n",
    "   - This is critical for determining relative positions\n",
    "\n",
    "3. LIST ALL CONSTRAINTS:\n",
    "   - Write down every constraint given in the problem\n",
    "   - Constraints might be: \\\"A is left of B\\\", \\\"C is opposite D\\\", \\\"E is between F and G\\\"\n",
    "   - Number the constraints for systematic application\n",
    "\n",
    "4. APPLY CONSTRAINTS STEP BY STEP:\n",
    "   - Start with the most definite constraint (exact positions or opposites)\n",
    "   - Build the arrangement progressively\n",
    "   - Use process of elimination for remaining positions\n",
    "\n",
    "5. VERIFY THE FINAL ARRANGEMENT:\n",
    "   - Check that all constraints are satisfied\n",
    "   - Ensure no contradictions exist\n",
    "   - Answer the specific question asked (who sits where, who is next to whom, etc.)\n",
    "\n",
    "CONSISTENCY CHECK:\n",
    "\n",
    "Before providing your final answer:\n",
    "- Ensure your reasoning logically leads to the answer you select\n",
    "- If reasoning concludes \\\"X is the uncle\\\", your answer should match the choice that says \\\"uncle\\\"\n",
    "- If reasoning concludes \\\"E sits at position 5\\\", your answer should match that choice\n",
    "- Do NOT contradict your own logical deduction\n",
    "- Double-check that the answer letter (A/B/C/D) corresponds to the correct option\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "\n",
    "Provide your response in exactly this format:\n",
    "\n",
    "answer: \\\"A\\\" or \\\"B\\\" or \\\"C\\\" or \\\"D\\\"\n",
    "reasoning: \\\"Step 1: [first logical step]. Step 2: [second logical step]. Step 3: [third logical step]. Step 4: [fourth logical step]. Step 5: [final conclusion].\\\"\n",
    "\n",
    "CRITICAL FORMAT REQUIREMENTS:\n",
    "- answer line must have: answer: followed by the letter in quotes\n",
    "- Only use capital letters A, B, C, or D\n",
    "- reasoning line must have: reasoning: followed by exactly 5 steps in a single string\n",
    "- Each step must start with \\\"Step N:\\\" where N is 1, 2, 3, 4, or 5\n",
    "- All 5 steps must be in ONE continuous string, separated by periods\n",
    "- Do NOT use an array or list format\n",
    "- Do NOT add any other fields or text\"\"\"\n",
    "\n",
    "print(f\"A-Agent system prompt: {len(A_AGENT_SYSTEM_PROMPT)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def format_question_with_choices(question, choices):\n",
    "    formatted = f\"{question}\\n\\nChoices:\\n\"\n",
    "    for choice in choices:\n",
    "        formatted += f\"{choice}\\n\"\n",
    "    return formatted.strip()\n",
    "\n",
    "def format_answer_simple(answer, reasoning):\n",
    "    response = f'answer: \"{answer}\"\\n'\n",
    "    response += f'reasoning: \"{reasoning}\"'\n",
    "    return response\n",
    "\n",
    "# Create A-Agent training examples\n",
    "a_agent_examples = []\n",
    "\n",
    "for q in all_questions:\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": A_AGENT_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": format_question_with_choices(\n",
    "                q.get('question', ''),\n",
    "                q.get('choices', [])\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": format_answer_simple(\n",
    "                q.get('answer', ''),\n",
    "                q.get('reasoning', '')\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    a_agent_examples.append({\"conversations\": conversation})\n",
    "\n",
    "print(f\"Created {len(a_agent_examples)} A-Agent training examples\")\n",
    "\n",
    "# Shuffle and split\n",
    "random.seed(42)\n",
    "random.shuffle(a_agent_examples)\n",
    "split_idx = int(len(a_agent_examples) * 0.9)\n",
    "\n",
    "a_train = a_agent_examples[:split_idx]\n",
    "a_val = a_agent_examples[split_idx:]\n",
    "\n",
    "print(f\"\\nSplit:\")\n",
    "print(f\"  Train: {len(a_train)}\")\n",
    "print(f\"  Val: {len(a_val)}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample A-Agent output:\")\n",
    "print(a_train[0]['conversations'][2]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "A_AGENT_CONFIG = {\n",
    "    \"model_name\": \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    \"max_seq_length\": 1536,\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"batch_size\": 8,\n",
    "    \"gradient_accumulation\": 2,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"warmup_steps\": 10,\n",
    "}\n",
    "\n",
    "print(\"A-Agent Model Configuration:\")\n",
    "for key, value in A_AGENT_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load A-Agent model\n",
    "print(\"\\nLoading A-Agent base model...\")\n",
    "\n",
    "a_model, a_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=A_AGENT_CONFIG[\"model_name\"],\n",
    "    max_seq_length=A_AGENT_CONFIG[\"max_seq_length\"],\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=False,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {A_AGENT_CONFIG['model_name']}\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "print(\"\\nAdding LoRA adapters...\")\n",
    "\n",
    "a_model = FastLanguageModel.get_peft_model(\n",
    "    a_model,\n",
    "    r=A_AGENT_CONFIG[\"lora_r\"],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=A_AGENT_CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "\n",
    "print(f\"LoRA adapters added (r={A_AGENT_CONFIG['lora_r']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "print(\"\\nLoading A-Agent datasets...\")\n",
    "\n",
    "a_train_dataset = Dataset.from_list(a_train)\n",
    "a_val_dataset = Dataset.from_list(a_val)\n",
    "\n",
    "print(f\"Train: {len(a_train_dataset)} examples\")\n",
    "print(f\"Val: {len(a_val_dataset)} examples\")\n",
    "\n",
    "# Set chat template for Llama\n",
    "a_tokenizer = get_chat_template(a_tokenizer, chat_template=\"llama-3.1\")\n",
    "\n",
    "if a_tokenizer.pad_token is None:\n",
    "    a_tokenizer.pad_token = a_tokenizer.eos_token\n",
    "    a_tokenizer.pad_token_id = a_tokenizer.eos_token_id\n",
    "\n",
    "# Formatting function\n",
    "def formatting_prompts_func_a(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    for convo in convos:\n",
    "        if isinstance(convo, list):\n",
    "            text = a_tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "print(\"\\nFormatting datasets...\")\n",
    "\n",
    "a_train_dataset = standardize_sharegpt(a_train_dataset)\n",
    "a_train_dataset = a_train_dataset.map(formatting_prompts_func_a, batched=True, remove_columns=a_train_dataset.column_names)\n",
    "a_train_dataset = a_train_dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "a_val_dataset = standardize_sharegpt(a_val_dataset)\n",
    "a_val_dataset = a_val_dataset.map(formatting_prompts_func_a, batched=True, remove_columns=a_val_dataset.column_names)\n",
    "a_val_dataset = a_val_dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "print(f\"Formatted {len(a_train_dataset)} train + {len(a_val_dataset)} val examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SFT trainer\n",
    "print(\"\\nSetting up A-Agent SFT trainer...\")\n",
    "\n",
    "a_output_dir = MODELS_DIR / \"a_agent_llama\"\n",
    "a_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "a_trainer = SFTTrainer(\n",
    "    model=a_model,\n",
    "    tokenizer=a_tokenizer,\n",
    "    train_dataset=a_train_dataset,\n",
    "    eval_dataset=a_val_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=A_AGENT_CONFIG[\"max_seq_length\"],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=a_tokenizer, padding=True),\n",
    "    packing=False,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=A_AGENT_CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=A_AGENT_CONFIG[\"batch_size\"],\n",
    "        gradient_accumulation_steps=A_AGENT_CONFIG[\"gradient_accumulation\"],\n",
    "        warmup_steps=A_AGENT_CONFIG[\"warmup_steps\"],\n",
    "        num_train_epochs=A_AGENT_CONFIG[\"num_epochs\"],\n",
    "        learning_rate=A_AGENT_CONFIG[\"learning_rate\"],\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=str(a_output_dir / \"checkpoints\"),\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train A-Agent Model (SFT Stage)\n",
    "print(\"\\nSTAGE 1: SFT TRAINING A-AGENT MODEL...\\n\")\n",
    "\n",
    "FastLanguageModel.for_training(a_model)\n",
    "a_trainer.train()\n",
    "\n",
    "print(\"\\nA-Agent SFT Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SFT model\n",
    "print(\"\\nSaving A-Agent SFT model...\")\n",
    "\n",
    "a_model.save_pretrained(str(a_output_dir / \"sft_lora\"))\n",
    "a_tokenizer.save_pretrained(str(a_output_dir / \"sft_lora\"))\n",
    "print(f\"LoRA adapters saved: {a_output_dir / 'sft_lora'}\")\n",
    "\n",
    "a_model.save_pretrained_merged(str(a_output_dir / \"sft_merged_16bit\"), a_tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"Merged 16bit saved: {a_output_dir / 'sft_merged_16bit'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare GRPO dataset\n",
    "print(\"\\nPreparing GRPO dataset...\")\n",
    "\n",
    "grpo_dataset = []\n",
    "\n",
    "for q in all_questions:\n",
    "    grpo_dataset.append({\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": A_AGENT_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": format_question_with_choices(q['question'], q['choices'])}\n",
    "        ],\n",
    "        \"ground_truth_answer\": q['answer'],\n",
    "        \"ground_truth_reasoning\": q['reasoning']\n",
    "    })\n",
    "\n",
    "grpo_dataset = Dataset.from_list(grpo_dataset)\n",
    "print(f\"GRPO dataset: {len(grpo_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO Reward Functions for A-Agent\n",
    "print(\"\\nDefining GRPO reward functions...\")\n",
    "\n",
    "# Regex patterns for parsing\n",
    "answer_pattern = re.compile(r'answer:\\s*\\\"([A-D])\\\"', re.IGNORECASE)\n",
    "reasoning_pattern = re.compile(r'reasoning:\\s*\\\"([^\\\"]+)\\\"', re.IGNORECASE)\n",
    "step_pattern = re.compile(r'Step\\s+(\\d+):', re.IGNORECASE)\n",
    "\n",
    "# Reward Function 1: Answer Correctness\n",
    "def reward_answer_correctness(prompts, completions, ground_truth_answer, **kwargs):\n",
    "    \"\"\"Reward if predicted answer matches ground truth\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for completion, true_answer in zip(completions, ground_truth_answer):\n",
    "        response = completion[0][\"content\"]\n",
    "        \n",
    "        # Extract answer\n",
    "        match = answer_pattern.search(response)\n",
    "        if match:\n",
    "            predicted_answer = match.group(1).upper()\n",
    "            # High reward for correct answer\n",
    "            score = 3.0 if predicted_answer == true_answer else -1.5\n",
    "        else:\n",
    "            # Penalty for not following format\n",
    "            score = -2.0\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Reward Function 2: Reasoning Quality (Step 1-5)\n",
    "def reward_reasoning_quality(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward if reasoning contains Step 1 through Step 5\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        score = 0.0\n",
    "        \n",
    "        # Extract reasoning\n",
    "        match = reasoning_pattern.search(response)\n",
    "        if match:\n",
    "            reasoning = match.group(1)\n",
    "            \n",
    "            # Find all steps\n",
    "            steps_found = step_pattern.findall(reasoning)\n",
    "            unique_steps = set(steps_found)\n",
    "            \n",
    "            # Check if we have exactly steps 1-5\n",
    "            expected_steps = {'1', '2', '3', '4', '5'}\n",
    "            \n",
    "            if expected_steps.issubset(unique_steps):\n",
    "                # Perfect - all 5 steps present\n",
    "                score = 2.0\n",
    "            elif len(unique_steps) >= 3:\n",
    "                # At least 3 steps, partial credit\n",
    "                score = 0.5\n",
    "            else:\n",
    "                # Too few steps\n",
    "                score = -1.0\n",
    "        else:\n",
    "            # No reasoning found\n",
    "            score = -1.5\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Reward Function 3: Format Adherence\n",
    "def reward_format_adherence(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward if response follows the exact format\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        score = 0.0\n",
    "        \n",
    "        # Check if both answer and reasoning are present\n",
    "        has_answer = answer_pattern.search(response) is not None\n",
    "        has_reasoning = reasoning_pattern.search(response) is not None\n",
    "        \n",
    "        if has_answer and has_reasoning:\n",
    "            score = 1.0\n",
    "        elif has_answer or has_reasoning:\n",
    "            score = 0.3\n",
    "        else:\n",
    "            score = -1.0\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "print(\"GRPO reward functions defined:\")\n",
    "print(\"  1. Answer Correctness (±3.0 points)\")\n",
    "print(\"  2. Reasoning Quality - Step 1-5 (±2.0 points)\")\n",
    "print(\"  3. Format Adherence (±1.0 points)\")\n",
    "print(\"  Total max reward: 6.0 points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max prompt length for GRPO\n",
    "max_seq_length = 1536\n",
    "max_prompt_len = max(grpo_dataset.map(\n",
    "    lambda x: {\"tokens\": a_tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt=True, tokenize=True)},\n",
    "    batched=True,\n",
    ").map(lambda x: {\"length\": len(x[\"tokens\"])}))[\"length\"])\n",
    "\n",
    "max_prompt_length = max_prompt_len + 10\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "print(f\"Max prompt length: {max_prompt_length}\")\n",
    "print(f\"Max completion length: {max_completion_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO configuration\n",
    "grpo_output_dir = MODELS_DIR / \"a_agent_llama_grpo\"\n",
    "grpo_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=12,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_completion_length,\n",
    "    max_steps=500,\n",
    "    save_steps=250,\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    "    output_dir=str(grpo_output_dir / \"checkpoints\"),\n",
    ")\n",
    "\n",
    "# Create GRPO trainer\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=a_model,\n",
    "    processing_class=a_tokenizer,\n",
    "    reward_funcs=[\n",
    "        reward_answer_correctness,\n",
    "        reward_reasoning_quality,\n",
    "        reward_format_adherence,\n",
    "    ],\n",
    "    args=grpo_config,\n",
    "    train_dataset=grpo_dataset,\n",
    ")\n",
    "\n",
    "print(\"GRPO trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with GRPO\n",
    "print(\"\\nSTAGE 2: GRPO TRAINING...\\n\")\n",
    "print(\"Watch the reward column increase over steps\\n\")\n",
    "\n",
    "grpo_trainer.train()\n",
    "\n",
    "print(\"\\nGRPO Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "print(\"\\nSaving final A-Agent model...\")\n",
    "\n",
    "a_model.save_pretrained(str(grpo_output_dir / \"lora\"))\n",
    "a_tokenizer.save_pretrained(str(grpo_output_dir / \"lora\"))\n",
    "print(f\"LoRA adapters saved: {grpo_output_dir / 'lora'}\")\n",
    "\n",
    "a_model.save_pretrained_merged(str(grpo_output_dir / \"merged_16bit\"), a_tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"Merged 16bit saved: {grpo_output_dir / 'merged_16bit'}\")\n",
    "\n",
    "print(f\"\\nA-Agent model ready: {grpo_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test A-Agent\n",
    "print(\"\\nTesting A-Agent Model...\\n\")\n",
    "\n",
    "FastLanguageModel.for_inference(a_model)\n",
    "\n",
    "test_questions = [\n",
    "    {\n",
    "        \"topic\": \"blood_relations\",\n",
    "        \"question\": \"Pointing to a woman, a man said, 'She is the daughter of my grandfather's only son.' How is the woman related to the man?\",\n",
    "        \"choices\": [\n",
    "            \"A) Sister\",\n",
    "            \"B) Cousin\",\n",
    "            \"C) Aunt\",\n",
    "            \"D) Mother\"\n",
    "        ],\n",
    "        \"correct_answer\": \"A\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"seating_arrangement\",\n",
    "        \"question\": \"Six friends A, B, C, D, E, F are sitting in a circle facing the center. A is to the immediate right of B. C is between D and E. F is not next to B. Who is to the immediate left of A?\",\n",
    "        \"choices\": [\n",
    "            \"A) C\",\n",
    "            \"B) D\",\n",
    "            \"C) E\",\n",
    "            \"D) F\"\n",
    "        ],\n",
    "        \"correct_answer\": \"B\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test_q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test Question {i}\")\n",
    "    print('='*60)\n",
    "\n",
    "    question_text = format_question_with_choices(test_q[\"question\"], test_q[\"choices\"])\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": A_AGENT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question_text}\n",
    "    ]\n",
    "\n",
    "    prompt = a_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = a_tokenizer(prompt, return_tensors=\"pt\").to(a_model.device)\n",
    "\n",
    "    outputs = a_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=a_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = a_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\nQuestion: {test_q['question']}\")\n",
    "    print(f\"\\nGenerated Answer:\\n\")\n",
    "    print(response)\n",
    "\n",
    "    # Extract answer and reasoning\n",
    "    answer_match = re.search(r'answer:\\s*\\\"([A-D])\\\"', response)\n",
    "    reasoning_match = re.search(r'reasoning:\\s*\\\"([^\\\"]+)\\\"', response)\n",
    "    \n",
    "    if answer_match:\n",
    "        predicted = answer_match.group(1)\n",
    "        correct = test_q[\"correct_answer\"]\n",
    "        status = \"MATCH\" if predicted == correct else \"WRONG\"\n",
    "        print(f\"\\nPredicted: {predicted} | Correct: {correct} | {status}\")\n",
    "    else:\n",
    "        print(\"\\nCould not extract answer\")\n",
    "    \n",
    "    if reasoning_match:\n",
    "        print(f\"Has reasoning: Yes\")\n",
    "    else:\n",
    "        print(\"Could not extract reasoning\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"A-Agent Testing Complete\")\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
