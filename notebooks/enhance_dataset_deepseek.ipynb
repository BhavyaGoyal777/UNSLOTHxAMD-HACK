{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Enhancement with DeepSeek-R1\n",
    "\n",
    "**Goal:** Generate MORE high-quality questions using existing curated JSONs\n",
    "\n",
    "**Model:** DeepSeek-R1-Distill-Llama-70B via vLLM\n",
    "\n",
    "**Process:**\n",
    "1. Load existing curated questions one by one\n",
    "2. Use each as example to generate variations\n",
    "3. Validate generated questions\n",
    "4. Save enhanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CURATED_DIR = Path(\"/Users/777bhavyagoyal/Developer/UNSLOTHxAMDxHACk/MAIN_CURATED_JSON\")\n",
    "OUTPUT_DIR = Path(\"/Users/777bhavyagoyal/Developer/UNSLOTHxAMDxHACk/ENHANCED_CURATED_JSON\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# vLLM settings from your config\n",
    "API_BASE = \"http://localhost:8001/v1\"\n",
    "MODEL = \"unsloth/DeepSeek-R1-Distill-Llama-70B\"\n",
    "\n",
    "# Generation settings\n",
    "DATASET_MULTIPLIER = 2  # Generate 2x the original dataset size\n",
    "TEMPERATURE = 0.4\n",
    "TOP_P = 0.95\n",
    "MAX_TOKENS = 4096\n",
    "SLEEP_TIME = 0.1  # From your config\n",
    "\n",
    "print(f\"âœ… Configuration:\")\n",
    "print(f\"  API: {API_BASE}\")\n",
    "print(f\"  Model: {MODEL}\")\n",
    "print(f\"  Dataset Multiplier: {DATASET_MULTIPLIER}x (will generate 2x original size)\")\n",
    "print(f\"  Temperature: {TEMPERATURE}\")\n",
    "print(f\"  Input: {CURATED_DIR}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CURATED_DIR = Path(\"/Users/777bhavyagoyal/Developer/UNSLOTHxAMDxHACk/MAIN_CURATED_JSON\")\n",
    "OUTPUT_DIR = Path(\"/Users/777bhavyagoyal/Developer/UNSLOTHxAMDxHACk/ENHANCED_CURATED_JSON\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# vLLM settings from your config\n",
    "API_BASE = \"http://localhost:8001/v1\"\n",
    "MODEL = \"unsloth/DeepSeek-R1-Distill-Llama-70B\"\n",
    "\n",
    "# Generation settings\n",
    "NUM_VARIATIONS_PER_QUESTION = 2  # Generate 2 variations per original question\n",
    "TEMPERATURE = 0.4\n",
    "TOP_P = 0.95\n",
    "MAX_TOKENS = 4096\n",
    "SLEEP_TIME = 0.1  # From your config\n",
    "\n",
    "print(f\"âœ… Configuration:\")\n",
    "print(f\"  API: {API_BASE}\")\n",
    "print(f\"  Model: {MODEL}\")\n",
    "print(f\"  Variations per question: {NUM_VARIATIONS_PER_QUESTION}\")\n",
    "print(f\"  Temperature: {TEMPERATURE}\")\n",
    "print(f\"  Input: {CURATED_DIR}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENHANCEMENT_PROMPT = \"\"\"Generate {num_new} NEW variations of this question for AMD AI Dev Day Hackathon.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "âœ“ Keep same topic ({topic})\n",
    "âœ“ Keep same difficulty level\n",
    "âœ“ Change entity names (use different letters/names)\n",
    "âœ“ Change numbers/positions/relationships\n",
    "âœ“ Keep same question structure\n",
    "âœ“ SELF-CONTAINED: Include ALL info needed to solve\n",
    "âœ“ NO coded relations (no symbols like * Ã— +)\n",
    "âœ“ Exactly 4 choices starting with \"A) \", \"B) \", \"C) \", \"D) \"\n",
    "âœ“ Answer is single capital letter: \"A\", \"B\", \"C\", or \"D\"\n",
    "âœ“ Reasoning as SINGLE STRING with 5 steps\n",
    "âœ“ 3-6 named entities\n",
    "âœ“ At least 3 unique constraints\n",
    "\n",
    "EXAMPLE QUESTION TO BASE ON:\n",
    "{example}\n",
    "\n",
    "Generate {num_new} NEW similar questions with:\n",
    "- Different entity names\n",
    "- Different numbers/positions\n",
    "- Different specific constraints\n",
    "- Same topic and difficulty\n",
    "\n",
    "Each question MUST have these exact 7 fields:\n",
    "{{\n",
    "  \"topic\": \"{topic}\",\n",
    "  \"question\": \"Complete self-contained question text\",\n",
    "  \"choices\": [\"A) option1\", \"B) option2\", \"C) option3\", \"D) option4\"],\n",
    "  \"answer\": \"A\" or \"B\" or \"C\" or \"D\",\n",
    "  \"explanation\": \"Brief explanation under 100 words\",\n",
    "  \"reasoning\": \"Step 1: ... Step 2: ... Step 3: ... Step 4: ... Step 5: ...\",\n",
    "  \"difficulty\": \"easy\" or \"medium\" or \"hard\"\n",
    "}}\n",
    "\n",
    "CRITICAL OUTPUT RULES:\n",
    "- Return ONLY JSON array: [{{}}, {{}}]\n",
    "- NO markdown code blocks\n",
    "- NO extra text before or after\n",
    "- After </think> tag output [\n",
    "- Reasoning must be SINGLE STRING not array\n",
    "- Exactly 4 choices per question\n",
    "\n",
    "Generate {num_new} new questions now:\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… Enhancement prompt ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vLLM API Call Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_vllm(prompt):\n",
    "    \"\"\"Call vLLM API and return generated questions\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{API_BASE}/completions\",\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"prompt\": prompt,\n",
    "                \"temperature\": TEMPERATURE,\n",
    "                \"top_p\": TOP_P,\n",
    "                \"max_tokens\": MAX_TOKENS\n",
    "            },\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        output = response.json()['choices'][0]['text']\n",
    "        \n",
    "        # Clean output - remove thinking tags if present\n",
    "        if '</think>' in output:\n",
    "            output = output.split('</think>')[-1].strip()\n",
    "        \n",
    "        # Remove markdown code blocks\n",
    "        output = output.replace('```json', '').replace('```', '').strip()\n",
    "        \n",
    "        # Extract JSON array\n",
    "        match = re.search(r'\\[.*\\]', output, re.DOTALL)\n",
    "        if match:\n",
    "            questions = json.loads(match.group(0))\n",
    "            return questions if isinstance(questions, list) else []\n",
    "        else:\n",
    "            print(f\"    âš ï¸  No JSON array found in output\")\n",
    "            return []\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"    âŒ API Error: {e}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"    âŒ JSON Parse Error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ Unexpected Error: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"âœ… API call function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate enhanced questions\n",
    "seed(42)\n",
    "\n",
    "# STEP 1: Count original questions\n",
    "total_original = len(all_questions)\n",
    "target_enhanced = total_original * DATASET_MULTIPLIER  # 2x original\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ðŸ“Š DATASET PLAN\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original questions: {total_original}\")\n",
    "print(f\"Target multiplier: {DATASET_MULTIPLIER}x\")\n",
    "print(f\"Target enhanced questions: {target_enhanced}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# STEP 2: Calculate how many variations per question\n",
    "variations_per_question = DATASET_MULTIPLIER  # 2 variations per question = 2x dataset\n",
    "\n",
    "enhanced_questions = []\n",
    "total_generated = 0\n",
    "total_valid = 0\n",
    "total_attempted = 0\n",
    "\n",
    "print(f\"ðŸ”„ Starting enhancement...\\n\")\n",
    "print(f\"Strategy: Generate {variations_per_question} variations per question\")\n",
    "print(f\"Processing {len(all_questions)} questions\")\n",
    "print(f\"Expected output: ~{target_enhanced} new questions\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx, original_q in enumerate(all_questions, 1):\n",
    "    topic = original_q.get('topic')\n",
    "    \n",
    "    print(f\"\\n[{idx}/{len(all_questions)}] Processing {topic} question...\")\n",
    "    print(f\"  Target: {variations_per_question} variations | Total so far: {total_valid}/{target_enhanced}\")\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = ENHANCEMENT_PROMPT.format(\n",
    "        num_new=variations_per_question,\n",
    "        topic=topic,\n",
    "        example=json.dumps(original_q, indent=2, ensure_ascii=False)\n",
    "    )\n",
    "    \n",
    "    # Call API\n",
    "    total_attempted += 1\n",
    "    new_questions = call_vllm(prompt)\n",
    "    \n",
    "    if new_questions:\n",
    "        total_generated += len(new_questions)\n",
    "        print(f\"  ðŸ“¥ Generated {len(new_questions)} questions\")\n",
    "        \n",
    "        # Validate each generated question\n",
    "        for q in new_questions:\n",
    "            is_valid, reason = validate_question(q, topic)\n",
    "            if is_valid:\n",
    "                enhanced_questions.append(q)\n",
    "                total_valid += 1\n",
    "                print(f\"  âœ… Valid question added (Total: {total_valid}/{target_enhanced})\")\n",
    "            else:\n",
    "                print(f\"  âŒ Invalid: {reason}\")\n",
    "    else:\n",
    "        print(f\"  âŒ No questions generated\")\n",
    "    \n",
    "    # Progress report every 50 questions\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROGRESS CHECKPOINT\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Processed: {idx}/{len(all_questions)} ({idx/len(all_questions)*100:.1f}%)\")\n",
    "        print(f\"Valid questions: {total_valid}/{target_enhanced} ({total_valid/target_enhanced*100:.1f}%)\")\n",
    "        print(f\"Success rate: {(total_valid/max(total_generated,1))*100:.1f}%\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    # Sleep to avoid overwhelming API\n",
    "    sleep(SLEEP_TIME)\n",
    "    \n",
    "    # Stop if we've reached target (optional - remove if you want exact multiplier)\n",
    "    # if total_valid >= target_enhanced:\n",
    "    #     print(f\"\\nâœ… Target reached! Stopping at {total_valid} questions\")\n",
    "    #     break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ðŸŽ‰ ENHANCEMENT COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original questions: {total_original}\")\n",
    "print(f\"Target enhanced: {target_enhanced}\")\n",
    "print(f\"API calls made: {total_attempted}\")\n",
    "print(f\"Questions generated: {total_generated}\")\n",
    "print(f\"Questions validated: {total_valid}\")\n",
    "print(f\"Success rate: {(total_valid/max(total_generated,1))*100:.1f}%\")\n",
    "print(f\"Target achievement: {(total_valid/target_enhanced)*100:.1f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_question(q, expected_topic):\n",
    "    \"\"\"Validate generated question meets all requirements\"\"\"\n",
    "    \n",
    "    # Check all required fields\n",
    "    required_fields = ['topic', 'question', 'choices', 'answer', 'explanation', 'reasoning', 'difficulty']\n",
    "    if not all(field in q for field in required_fields):\n",
    "        return False, \"Missing required fields\"\n",
    "    \n",
    "    # Check topic matches\n",
    "    if q.get('topic') != expected_topic:\n",
    "        return False, f\"Topic mismatch: expected {expected_topic}, got {q.get('topic')}\"\n",
    "    \n",
    "    # Check exactly 4 choices\n",
    "    if not isinstance(q.get('choices'), list) or len(q.get('choices')) != 4:\n",
    "        return False, \"Must have exactly 4 choices\"\n",
    "    \n",
    "    # Check choices format\n",
    "    choice_prefixes = ['A)', 'B)', 'C)', 'D)']\n",
    "    for i, choice in enumerate(q.get('choices', [])):\n",
    "        if not choice.startswith(choice_prefixes[i]):\n",
    "            return False, f\"Choice {i+1} must start with {choice_prefixes[i]}\"\n",
    "    \n",
    "    # Check answer format\n",
    "    if q.get('answer') not in ['A', 'B', 'C', 'D']:\n",
    "        return False, \"Answer must be A, B, C, or D\"\n",
    "    \n",
    "    # Check reasoning is string not array\n",
    "    if not isinstance(q.get('reasoning'), str):\n",
    "        return False, \"Reasoning must be a single string, not array\"\n",
    "    \n",
    "    # Check reasoning has 5 steps\n",
    "    reasoning = q.get('reasoning', '')\n",
    "    step_count = sum(1 for i in range(1, 6) if f'Step {i}:' in reasoning)\n",
    "    if step_count < 5:\n",
    "        return False, f\"Reasoning must have 5 steps, found {step_count}\"\n",
    "    \n",
    "    # Check difficulty\n",
    "    if q.get('difficulty') not in ['easy', 'medium', 'hard']:\n",
    "        return False, \"Difficulty must be easy, medium, or hard\"\n",
    "    \n",
    "    # Check question is not too short (self-contained check)\n",
    "    if len(q.get('question', '')) < 50:\n",
    "        return False, \"Question too short, likely not self-contained\"\n",
    "    \n",
    "    return True, \"Valid\"\n",
    "\n",
    "print(\"âœ… Validation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Existing Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all curated questions from JSON files\n",
    "all_questions = []\n",
    "json_files = sorted(CURATED_DIR.glob(\"*.json\"))\n",
    "\n",
    "print(f\"ðŸ“‚ Found {len(json_files)} JSON files in {CURATED_DIR}\\n\")\n",
    "\n",
    "for json_file in json_files:\n",
    "    try:\n",
    "        with open(json_file, 'r') as f:\n",
    "            questions = json.load(f)\n",
    "            \n",
    "        # Filter valid questions\n",
    "        valid = []\n",
    "        for q in questions:\n",
    "            if (len(q.get('choices', [])) == 4 and \n",
    "                q.get('answer', '') in ['A', 'B', 'C', 'D'] and\n",
    "                q.get('topic') in ['blood_relations', 'seating_arrangement']):\n",
    "                valid.append(q)\n",
    "        \n",
    "        all_questions.extend(valid)\n",
    "        print(f\"  âœ… {json_file.name}: Loaded {len(valid)} valid questions\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ {json_file.name}: Error - {e}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total curated questions loaded: {len(all_questions)}\")\n",
    "\n",
    "# Group by topic\n",
    "by_topic = {}\n",
    "for q in all_questions:\n",
    "    topic = q.get('topic')\n",
    "    if topic not in by_topic:\n",
    "        by_topic[topic] = []\n",
    "    by_topic[topic].append(q)\n",
    "\n",
    "print(f\"\\nðŸ“‹ Questions by topic:\")\n",
    "for topic, questions in by_topic.items():\n",
    "    print(f\"  {topic}: {len(questions)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Enhanced Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate enhanced questions\n",
    "seed(42)\n",
    "\n",
    "enhanced_questions = []\n",
    "total_generated = 0\n",
    "total_valid = 0\n",
    "total_attempted = 0\n",
    "\n",
    "print(f\"\\nðŸ”„ Starting enhancement...\\n\")\n",
    "print(f\"Processing {len(all_questions)} questions\")\n",
    "print(f\"Generating {NUM_VARIATIONS_PER_QUESTION} variations per question\")\n",
    "print(f\"Expected total: {len(all_questions) * NUM_VARIATIONS_PER_QUESTION} new questions\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx, original_q in enumerate(all_questions, 1):\n",
    "    topic = original_q.get('topic')\n",
    "    \n",
    "    print(f\"\\n[{idx}/{len(all_questions)}] Processing {topic} question...\")\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = ENHANCEMENT_PROMPT.format(\n",
    "        num_new=NUM_VARIATIONS_PER_QUESTION,\n",
    "        topic=topic,\n",
    "        example=json.dumps(original_q, indent=2, ensure_ascii=False)\n",
    "    )\n",
    "    \n",
    "    # Call API\n",
    "    total_attempted += 1\n",
    "    new_questions = call_vllm(prompt)\n",
    "    \n",
    "    if new_questions:\n",
    "        total_generated += len(new_questions)\n",
    "        print(f\"  ðŸ“¥ Generated {len(new_questions)} questions\")\n",
    "        \n",
    "        # Validate each generated question\n",
    "        for q in new_questions:\n",
    "            is_valid, reason = validate_question(q, topic)\n",
    "            if is_valid:\n",
    "                enhanced_questions.append(q)\n",
    "                total_valid += 1\n",
    "                print(f\"  âœ… Valid question added\")\n",
    "            else:\n",
    "                print(f\"  âŒ Invalid: {reason}\")\n",
    "    else:\n",
    "        print(f\"  âŒ No questions generated\")\n",
    "    \n",
    "    # Progress report every 10 questions\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROGRESS: {idx}/{len(all_questions)} processed\")\n",
    "        print(f\"Generated: {total_generated} | Valid: {total_valid} | Success rate: {(total_valid/max(total_generated,1))*100:.1f}%\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    # Sleep to avoid overwhelming API\n",
    "    sleep(SLEEP_TIME)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ðŸŽ‰ ENHANCEMENT COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original questions: {len(all_questions)}\")\n",
    "print(f\"API calls made: {total_attempted}\")\n",
    "print(f\"Questions generated: {total_generated}\")\n",
    "print(f\"Questions validated: {total_valid}\")\n",
    "print(f\"Success rate: {(total_valid/max(total_generated,1))*100:.1f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Enhanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all enhanced questions\n",
    "output_file = OUTPUT_DIR / \"enhanced_questions.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(enhanced_questions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Saved {len(enhanced_questions)} enhanced questions\")\n",
    "print(f\"   File: {output_file}\")\n",
    "\n",
    "# Save by topic\n",
    "by_topic_enhanced = {}\n",
    "for q in enhanced_questions:\n",
    "    topic = q.get('topic')\n",
    "    if topic not in by_topic_enhanced:\n",
    "        by_topic_enhanced[topic] = []\n",
    "    by_topic_enhanced[topic].append(q)\n",
    "\n",
    "print(f\"\\nðŸ“ Saving by topic:\")\n",
    "for topic, questions in by_topic_enhanced.items():\n",
    "    topic_file = OUTPUT_DIR / f\"enhanced_{topic}.json\"\n",
    "    with open(topic_file, 'w') as f:\n",
    "        json.dump(questions, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"  âœ… {topic}: {len(questions)} questions â†’ {topic_file.name}\")\n",
    "\n",
    "# Combine original + enhanced\n",
    "combined = all_questions + enhanced_questions\n",
    "combined_file = OUTPUT_DIR / \"combined_curated_enhanced.json\"\n",
    "with open(combined_file, 'w') as f:\n",
    "    json.dump(combined, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nâœ… Combined dataset saved\")\n",
    "print(f\"   File: {combined_file}\")\n",
    "print(f\"   Total: {len(combined)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ðŸ“Š FINAL SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nðŸ“ˆ Dataset Growth:\")\n",
    "print(f\"  Original curated: {len(all_questions)}\")\n",
    "print(f\"  Enhanced generated: {len(enhanced_questions)}\")\n",
    "print(f\"  Total combined: {len(combined)}\")\n",
    "print(f\"  Growth multiplier: {len(combined)/len(all_questions):.2f}x\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ By Topic (Original â†’ Enhanced):\")\n",
    "for topic in by_topic.keys():\n",
    "    orig_count = len(by_topic.get(topic, []))\n",
    "    enh_count = len(by_topic_enhanced.get(topic, []))\n",
    "    total = orig_count + enh_count\n",
    "    print(f\"  {topic}:\")\n",
    "    print(f\"    Original: {orig_count}\")\n",
    "    print(f\"    Enhanced: {enh_count}\")\n",
    "    print(f\"    Total: {total} ({total/orig_count:.2f}x growth)\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Output Files:\")\n",
    "print(f\"  {OUTPUT_DIR}/\")\n",
    "print(f\"    â”œâ”€â”€ enhanced_questions.json ({len(enhanced_questions)} questions)\")\n",
    "print(f\"    â”œâ”€â”€ combined_curated_enhanced.json ({len(combined)} questions)\")\n",
    "for topic in by_topic_enhanced.keys():\n",
    "    print(f\"    â””â”€â”€ enhanced_{topic}.json ({len(by_topic_enhanced[topic])} questions)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ðŸŽŠ Enhancement Complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample enhanced questions\n",
    "if enhanced_questions:\n",
    "    print(\"\\nðŸ“ Sample Enhanced Questions:\\n\")\n",
    "    \n",
    "    samples = sample(enhanced_questions, min(3, len(enhanced_questions)))\n",
    "    \n",
    "    for i, q in enumerate(samples, 1):\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Sample {i}: {q.get('topic')} ({q.get('difficulty')})\")\n",
    "        print('='*60)\n",
    "        print(json.dumps(q, indent=2, ensure_ascii=False))\n",
    "        print()\n",
    "        \n",
    "        # Quick validation check\n",
    "        is_valid, reason = validate_question(q, q.get('topic'))\n",
    "        if is_valid:\n",
    "            print(\"âœ… Validation: PASS\")\n",
    "        else:\n",
    "            print(f\"âŒ Validation: FAIL - {reason}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No enhanced questions generated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
