{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Agent Training - Llama-3.2-3B with GRPO\n",
    "\n",
    "Model: Llama-3.2-3B-Instruct (3 billion parameters)\n",
    "\n",
    "Task: Generate logical reasoning questions\n",
    "\n",
    "Training: SFT + GRPO (reward for valid JSON structure)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt\n",
    "from trl import SFTConfig, SFTTrainer, GRPOConfig, GRPOTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "print(\"Imports successful\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURATED_DIR = Path(\"MAIN_CURATED_JSON\")\n",
    "MODELS_DIR = Path(\"FINAL_MODELS\")\n",
    "\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Directories ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all curated questions and remove difficulty field\n",
    "all_questions = []\n",
    "curated_files = sorted(CURATED_DIR.glob(\"*.json\"))\n",
    "\n",
    "for file_path in curated_files:\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            questions = json.load(f)\n",
    "            for q in questions:\n",
    "                if len(q.get('choices', [])) == 4 and q.get('answer', '') in ['A', 'B', 'C', 'D']:\n",
    "                    # Explicitly remove difficulty field\n",
    "                    all_questions.append({\n",
    "                        \"topic\": q['topic'],\n",
    "                        \"question\": q['question'],\n",
    "                        \"choices\": q['choices'],\n",
    "                        \"answer\": q['answer'],\n",
    "                        \"explanation\": q['explanation'],\n",
    "                        \"reasoning\": q['reasoning']\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path.name}: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(all_questions)} valid questions\")\n",
    "print(f\"\\nSample question (NO difficulty field):\")\n",
    "print(json.dumps(all_questions[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_AGENT_SYSTEM_PROMPT = \"\"\"You are Q-Agent, an expert question generator for the AMD AI Dev Day Hackathon.\n",
    "\n",
    "Your task is to generate high-quality logical reasoning questions for competitive exam preparation.\n",
    "\n",
    "ALLOWED TOPICS:\n",
    "Only generate questions on these two topics:\n",
    "1. blood_relations - Family relationship puzzles and genealogy problems\n",
    "2. seating_arrangement - Linear row seating OR circular table seating\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "\n",
    "1. ENTITY COUNT:\n",
    "   - Include 3-6 named entities (people, objects, or positions) per question\n",
    "   - Use distinct names or letters (A, B, C, D, E or Ram, Sita, John, Mary)\n",
    "   - Each entity must play a clear role in the problem\n",
    "\n",
    "2. CONSTRAINTS AND RELATIONS:\n",
    "   - Provide at least 3 unique relations or constraints\n",
    "   - Each constraint must add information to solve the problem\n",
    "   - Constraints should not contradict each other\n",
    "\n",
    "3. SELF-CONTAINED QUESTIONS:\n",
    "   - Include ALL information needed to solve the problem within the question text\n",
    "   - Do NOT assume any external context or prior knowledge beyond basic logic\n",
    "   - State ALL relationships, positions, and constraints explicitly\n",
    "   - BAD: \\\"How is X related to Y?\\\" (missing context about who X and Y are)\n",
    "   - GOOD: \\\"X is the son of Y. Y is the brother of Z. Z is the father of W. How is X related to W?\\\"\n",
    "\n",
    "4. NO CODED RELATIONS:\n",
    "   - Do NOT use mathematical symbols to represent relationships\n",
    "   - Avoid: *, +, -, ×, %, $, #, @ to indicate relations\n",
    "   - Use plain English descriptions only\n",
    "   - BAD: \\\"K * J × L + M\\\" to represent family relations\n",
    "   - GOOD: \\\"K is the father of J. J is the mother of L. M is the son of L.\\\"\n",
    "\n",
    "5. MULTIPLE CHOICE FORMAT:\n",
    "   - Provide exactly 4 choices labeled A, B, C, D\n",
    "   - Format: [\\\"A) option1\\\", \\\"B) option2\\\", \\\"C) option3\\\", \\\"D) option4\\\"]\n",
    "   - Each choice must start with the letter, followed by parenthesis and space\n",
    "   - Only ONE choice should be correct\n",
    "   - Other three choices should be plausible but incorrect distractors\n",
    "\n",
    "6. ANSWER FORMAT:\n",
    "   - Provide single letter answer: \\\"A\\\", \\\"B\\\", \\\"C\\\", or \\\"D\\\"\n",
    "   - Nothing else, just the capital letter\n",
    "\n",
    "7. EXPLANATION:\n",
    "   - Provide brief explanation (under 100 words) why the answer is correct\n",
    "   - Focus on the key logical step that leads to the answer\n",
    "\n",
    "8. REASONING:\n",
    "   - Provide detailed step-by-step reasoning\n",
    "   - Format as SINGLE STRING with exactly 5 steps\n",
    "   - \\\"Step 1: [description]. Step 2: [description]. Step 3: [description]. Step 4: [description]. Step 5: [description].\\\"\n",
    "   - Each step should build on previous steps to reach the answer\n",
    "\n",
    "BLOOD RELATIONS GUIDELINES:\n",
    "\n",
    "Use clear family relationship terms:\n",
    "- Direct relations: father, mother, son, daughter, brother, sister\n",
    "- Extended relations: grandfather, grandmother, uncle, aunt, cousin, nephew, niece\n",
    "- In-law relations: brother-in-law, sister-in-law, father-in-law, mother-in-law\n",
    "\n",
    "Structure:\n",
    "- Start with 3-5 relationship statements establishing the family tree\n",
    "- End with a question asking about a specific relationship\n",
    "- Ensure the chain of relationships logically connects to the answer\n",
    "\n",
    "Example pattern:\n",
    "\\\"A is the son of B. B is the brother of C. C is the father of D. How is A related to D?\\\"\n",
    "\n",
    "SEATING ARRANGEMENT GUIDELINES:\n",
    "\n",
    "Linear Arrangements:\n",
    "- Format: \\\"P, Q, R, S, T sit in a row from left to right\\\"\n",
    "- Use positional terms: left, right, leftmost, rightmost, between, adjacent\n",
    "- Example: \\\"P sits third from the left. Q sits between P and R.\\\"\n",
    "\n",
    "Circular Arrangements:\n",
    "- Format: \\\"A, B, C, D, E sit around a circular table\\\"\n",
    "- MUST specify facing direction: \\\"facing the center\\\" or \\\"facing outward\\\"\n",
    "- Use terms: clockwise, anticlockwise, opposite, between, next to\n",
    "- Example: \\\"A sits to the immediate right of B. C sits opposite to D.\\\"\n",
    "\n",
    "Restrictions:\n",
    "- Use ONLY linear rows OR circular tables\n",
    "- Do NOT use: rectangular tables, square arrangements, compass directions (North, South, East, West)\n",
    "- Do NOT use: boxes, grids, or complex 2D layouts\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "\n",
    "Generate a single JSON object with exactly 6 fields (NO difficulty field):\n",
    "\n",
    "{\n",
    "  \\\"topic\\\": \\\"blood_relations\\\" or \\\"seating_arrangement\\\",\n",
    "  \\\"question\\\": \\\"Complete self-contained problem statement with all necessary information\\\",\n",
    "  \\\"choices\\\": [\\\"A) option1\\\", \\\"B) option2\\\", \\\"C) option3\\\", \\\"D) option4\\\"],\n",
    "  \\\"answer\\\": \\\"A\\\" or \\\"B\\\" or \\\"C\\\" or \\\"D\\\",\n",
    "  \\\"explanation\\\": \\\"Brief justification under 100 words explaining why the answer is correct\\\",\n",
    "  \\\"reasoning\\\": \\\"Step 1: [description]. Step 2: [description]. Step 3: [description]. Step 4: [description]. Step 5: [description].\\\"\n",
    "}\n",
    "\n",
    "IMPORTANT:\n",
    "- Do NOT include a difficulty field\n",
    "- Difficulty emerges naturally from entity count and relation complexity\n",
    "- Return ONLY valid JSON, no markdown code blocks\n",
    "- No explanatory text before or after the JSON object\"\"\"\n",
    "\n",
    "print(f\"Q-Agent system prompt: {len(Q_AGENT_SYSTEM_PROMPT)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Q-Agent training examples\n",
    "q_agent_examples = []\n",
    "\n",
    "for q in all_questions:\n",
    "    topic_readable = q['topic'].replace('_', ' ')\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": Q_AGENT_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Generate a {topic_readable} question with 3-6 entities, multiple choice options, and step-by-step reasoning.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": json.dumps(q, ensure_ascii=False)\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    q_agent_examples.append({\"conversations\": conversation})\n",
    "\n",
    "print(f\"Created {len(q_agent_examples)} Q-Agent training examples\")\n",
    "\n",
    "# Shuffle and split\n",
    "random.seed(42)\n",
    "random.shuffle(q_agent_examples)\n",
    "split_idx = int(len(q_agent_examples) * 0.9)\n",
    "\n",
    "q_train = q_agent_examples[:split_idx]\n",
    "q_val = q_agent_examples[split_idx:]\n",
    "\n",
    "print(f\"\\nSplit:\")\n",
    "print(f\"  Train: {len(q_train)}\")\n",
    "print(f\"  Val: {len(q_val)}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample Q-Agent output:\")\n",
    "print(q_train[0]['conversations'][2]['content'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "Q_AGENT_CONFIG = {\n",
    "    \"model_name\": \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_accumulation\": 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"warmup_steps\": 10,\n",
    "}\n",
    "\n",
    "print(\"Q-Agent Model Configuration:\")\n",
    "for key, value in Q_AGENT_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Q-Agent model\n",
    "print(\"\\nLoading Q-Agent base model...\")\n",
    "\n",
    "q_model, q_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=Q_AGENT_CONFIG[\"model_name\"],\n",
    "    max_seq_length=Q_AGENT_CONFIG[\"max_seq_length\"],\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=False,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {Q_AGENT_CONFIG['model_name']}\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "print(\"\\nAdding LoRA adapters...\")\n",
    "\n",
    "q_model = FastLanguageModel.get_peft_model(\n",
    "    q_model,\n",
    "    r=Q_AGENT_CONFIG[\"lora_r\"],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=Q_AGENT_CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "\n",
    "print(f\"LoRA adapters added (r={Q_AGENT_CONFIG['lora_r']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "print(\"\\nLoading Q-Agent datasets...\")\n",
    "\n",
    "q_train_dataset = Dataset.from_list(q_train)\n",
    "q_val_dataset = Dataset.from_list(q_val)\n",
    "\n",
    "print(f\"Train: {len(q_train_dataset)} examples\")\n",
    "print(f\"Val: {len(q_val_dataset)} examples\")\n",
    "\n",
    "# Set chat template for Llama\n",
    "q_tokenizer = get_chat_template(q_tokenizer, chat_template=\"llama-3.1\")\n",
    "\n",
    "if q_tokenizer.pad_token is None:\n",
    "    q_tokenizer.pad_token = q_tokenizer.eos_token\n",
    "    q_tokenizer.pad_token_id = q_tokenizer.eos_token_id\n",
    "\n",
    "# Formatting function\n",
    "def formatting_prompts_func_q(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    for convo in convos:\n",
    "        if isinstance(convo, list):\n",
    "            text = q_tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "print(\"\\nFormatting datasets...\")\n",
    "\n",
    "q_train_dataset = standardize_sharegpt(q_train_dataset)\n",
    "q_train_dataset = q_train_dataset.map(formatting_prompts_func_q, batched=True, remove_columns=q_train_dataset.column_names)\n",
    "q_train_dataset = q_train_dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "q_val_dataset = standardize_sharegpt(q_val_dataset)\n",
    "q_val_dataset = q_val_dataset.map(formatting_prompts_func_q, batched=True, remove_columns=q_val_dataset.column_names)\n",
    "q_val_dataset = q_val_dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "print(f\"Formatted {len(q_train_dataset)} train + {len(q_val_dataset)} val examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SFT trainer\n",
    "print(\"\\nSetting up Q-Agent SFT trainer...\")\n",
    "\n",
    "q_output_dir = MODELS_DIR / \"q_agent_llama\"\n",
    "q_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "q_trainer = SFTTrainer(\n",
    "    model=q_model,\n",
    "    tokenizer=q_tokenizer,\n",
    "    train_dataset=q_train_dataset,\n",
    "    eval_dataset=q_val_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=Q_AGENT_CONFIG[\"max_seq_length\"],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=q_tokenizer, padding=True),\n",
    "    packing=False,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=Q_AGENT_CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=Q_AGENT_CONFIG[\"batch_size\"],\n",
    "        gradient_accumulation_steps=Q_AGENT_CONFIG[\"gradient_accumulation\"],\n",
    "        warmup_steps=Q_AGENT_CONFIG[\"warmup_steps\"],\n",
    "        num_train_epochs=Q_AGENT_CONFIG[\"num_epochs\"],\n",
    "        learning_rate=Q_AGENT_CONFIG[\"learning_rate\"],\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=str(q_output_dir / \"checkpoints\"),\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Q-Agent Model (SFT Stage)\n",
    "print(\"\\nSTAGE 1: SFT TRAINING Q-AGENT MODEL...\\n\")\n",
    "\n",
    "FastLanguageModel.for_training(q_model)\n",
    "q_trainer.train()\n",
    "\n",
    "print(\"\\nQ-Agent SFT Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SFT model\n",
    "print(\"\\nSaving Q-Agent SFT model...\")\n",
    "\n",
    "q_model.save_pretrained(str(q_output_dir / \"sft_lora\"))\n",
    "q_tokenizer.save_pretrained(str(q_output_dir / \"sft_lora\"))\n",
    "print(f\"LoRA adapters saved: {q_output_dir / 'sft_lora'}\")\n",
    "\n",
    "q_model.save_pretrained_merged(str(q_output_dir / \"sft_merged_16bit\"), q_tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"Merged 16bit saved: {q_output_dir / 'sft_merged_16bit'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare GRPO dataset\n",
    "print(\"\\nPreparing GRPO dataset...\")\n",
    "\n",
    "grpo_dataset = []\n",
    "\n",
    "for q in all_questions:\n",
    "    topic_readable = q['topic'].replace('_', ' ')\n",
    "    \n",
    "    grpo_dataset.append({\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": Q_AGENT_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"Generate a {topic_readable} question with 3-6 entities, multiple choice options, and step-by-step reasoning.\"}\n",
    "        ],\n",
    "        \"ground_truth\": q\n",
    "    })\n",
    "\n",
    "grpo_dataset = Dataset.from_list(grpo_dataset)\n",
    "print(f\"GRPO dataset: {len(grpo_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO Reward Functions for Q-Agent\n",
    "print(\"\\nDefining GRPO reward functions...\")\n",
    "\n",
    "# Reward Function 1: Valid JSON Structure\n",
    "def reward_json_validity(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward if response is valid JSON\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        \n",
    "        try:\n",
    "            parsed = json.loads(response)\n",
    "            # Valid JSON gets high reward\n",
    "            score = 3.0\n",
    "        except json.JSONDecodeError:\n",
    "            # Try to extract JSON from text\n",
    "            if '{' in response and '}' in response:\n",
    "                json_start = response.find('{')\n",
    "                json_end = response.rfind('}') + 1\n",
    "                try:\n",
    "                    parsed = json.loads(response[json_start:json_end])\n",
    "                    score = 2.0  # Partial credit for extractable JSON\n",
    "                except:\n",
    "                    score = -2.0  # Penalty for invalid JSON\n",
    "            else:\n",
    "                score = -3.0  # Strong penalty for no JSON\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Reward Function 2: Required Fields Present\n",
    "def reward_required_fields(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward if all required fields are present\"\"\"\n",
    "    scores = []\n",
    "    required_fields = ['topic', 'question', 'choices', 'answer', 'explanation', 'reasoning']\n",
    "    \n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        \n",
    "        try:\n",
    "            # Try direct parse\n",
    "            parsed = json.loads(response)\n",
    "        except:\n",
    "            # Try extraction\n",
    "            if '{' in response and '}' in response:\n",
    "                json_start = response.find('{')\n",
    "                json_end = response.rfind('}') + 1\n",
    "                try:\n",
    "                    parsed = json.loads(response[json_start:json_end])\n",
    "                except:\n",
    "                    parsed = None\n",
    "            else:\n",
    "                parsed = None\n",
    "        \n",
    "        if parsed:\n",
    "            # Count present fields\n",
    "            present = sum(1 for field in required_fields if field in parsed)\n",
    "            \n",
    "            if present == 6:\n",
    "                score = 2.0  # All fields present\n",
    "            elif present >= 4:\n",
    "                score = 0.5  # Most fields present\n",
    "            else:\n",
    "                score = -1.0  # Too few fields\n",
    "            \n",
    "            # Penalty if difficulty field is present (we don't want it)\n",
    "            if 'difficulty' in parsed:\n",
    "                score -= 1.0\n",
    "        else:\n",
    "            score = -1.5\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Reward Function 3: Format Correctness\n",
    "def reward_format_correctness(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward if choices and answer format are correct\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        \n",
    "        try:\n",
    "            parsed = json.loads(response)\n",
    "        except:\n",
    "            if '{' in response and '}' in response:\n",
    "                json_start = response.find('{')\n",
    "                json_end = response.rfind('}') + 1\n",
    "                try:\n",
    "                    parsed = json.loads(response[json_start:json_end])\n",
    "                except:\n",
    "                    parsed = None\n",
    "            else:\n",
    "                parsed = None\n",
    "        \n",
    "        if parsed:\n",
    "            score = 0.0\n",
    "            \n",
    "            # Check choices format\n",
    "            choices = parsed.get('choices', [])\n",
    "            if isinstance(choices, list) and len(choices) == 4:\n",
    "                # Check if choices start with A), B), C), D)\n",
    "                if all(c[0] in 'ABCD' and c[1:3] == ') ' for c in choices if len(c) > 2):\n",
    "                    score += 1.0\n",
    "            \n",
    "            # Check answer format\n",
    "            answer = parsed.get('answer', '')\n",
    "            if isinstance(answer, str) and answer in ['A', 'B', 'C', 'D']:\n",
    "                score += 1.0\n",
    "            \n",
    "            # Check reasoning has steps\n",
    "            reasoning = parsed.get('reasoning', '')\n",
    "            if 'Step 1' in reasoning and 'Step 5' in reasoning:\n",
    "                score += 1.0\n",
    "        else:\n",
    "            score = -1.5\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "print(\"GRPO reward functions defined:\")\n",
    "print(\"  1. JSON Validity (±3.0 points)\")\n",
    "print(\"  2. Required Fields (±2.0 points)\")\n",
    "print(\"  3. Format Correctness (±3.0 points)\")\n",
    "print(\"  Total max reward: 8.0 points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max prompt length for GRPO\n",
    "max_seq_length = 2048\n",
    "max_prompt_len = max(grpo_dataset.map(\n",
    "    lambda x: {\"tokens\": q_tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt=True, tokenize=True)},\n",
    "    batched=True,\n",
    ").map(lambda x: {\"length\": len(x[\"tokens\"])}))[\"length\"])\n",
    "\n",
    "max_prompt_length = max_prompt_len + 10\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "print(f\"Max prompt length: {max_prompt_length}\")\n",
    "print(f\"Max completion length: {max_completion_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO configuration\n",
    "grpo_output_dir = MODELS_DIR / \"q_agent_llama_grpo\"\n",
    "grpo_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=8,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_completion_length,\n",
    "    max_steps=500,\n",
    "    save_steps=250,\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    "    output_dir=str(grpo_output_dir / \"checkpoints\"),\n",
    ")\n",
    "\n",
    "# Create GRPO trainer\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=q_model,\n",
    "    processing_class=q_tokenizer,\n",
    "    reward_funcs=[\n",
    "        reward_json_validity,\n",
    "        reward_required_fields,\n",
    "        reward_format_correctness,\n",
    "    ],\n",
    "    args=grpo_config,\n",
    "    train_dataset=grpo_dataset,\n",
    ")\n",
    "\n",
    "print(\"GRPO trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with GRPO\n",
    "print(\"\\nSTAGE 2: GRPO TRAINING...\\n\")\n",
    "print(\"Watch the reward column increase over steps\\n\")\n",
    "\n",
    "grpo_trainer.train()\n",
    "\n",
    "print(\"\\nGRPO Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "print(\"\\nSaving final Q-Agent model...\")\n",
    "\n",
    "q_model.save_pretrained(str(grpo_output_dir / \"lora\"))\n",
    "q_tokenizer.save_pretrained(str(grpo_output_dir / \"lora\"))\n",
    "print(f\"LoRA adapters saved: {grpo_output_dir / 'lora'}\")\n",
    "\n",
    "q_model.save_pretrained_merged(str(grpo_output_dir / \"merged_16bit\"), q_tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"Merged 16bit saved: {grpo_output_dir / 'merged_16bit'}\")\n",
    "\n",
    "print(f\"\\nQ-Agent model ready: {grpo_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Q-Agent\n",
    "print(\"\\nTesting Q-Agent Model...\\n\")\n",
    "\n",
    "FastLanguageModel.for_inference(q_model)\n",
    "\n",
    "test_topics = [\n",
    "    \"blood relations\",\n",
    "    \"seating arrangement\"\n",
    "]\n",
    "\n",
    "for topic in test_topics:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test: Generate {topic} question\")\n",
    "    print('='*60)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": Q_AGENT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Generate a {topic} question with 3-6 entities, multiple choice options, and step-by-step reasoning.\"}\n",
    "    ]\n",
    "\n",
    "    prompt = q_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = q_tokenizer(prompt, return_tensors=\"pt\").to(q_model.device)\n",
    "\n",
    "    outputs = q_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=q_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = q_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\nGenerated JSON:\\n\")\n",
    "    print(response[:500])\n",
    "    \n",
    "    # Try to parse JSON\n",
    "    try:\n",
    "        q_json = json.loads(response)\n",
    "        print(f\"\\nValid JSON\")\n",
    "        print(f\"Topic: {q_json.get('topic')}\")\n",
    "        print(f\"Choices: {len(q_json.get('choices', []))}\")\n",
    "        print(f\"Answer: {q_json.get('answer')}\")\n",
    "        print(f\"Has reasoning: {'reasoning' in q_json}\")\n",
    "        print(f\"NO difficulty: {'difficulty' not in q_json}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCould not parse JSON: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Q-Agent Testing Complete\")\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
