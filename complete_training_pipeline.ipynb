{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Training Pipeline: Question & Answer Generation Models\n",
    "\n",
    "This notebook trains **2 specialized models** for logical reasoning:\n",
    "\n",
    "1. **Question Generation Model** - Generates high-quality logical reasoning questions\n",
    "2. **Answer Generation Model** - Solves logical reasoning questions with step-by-step reasoning\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Dataset Stats\n",
    "- **Total Questions**: 638 (98.9% quality)\n",
    "- **Blood Relations**: 401 (62.9%)\n",
    "- **Seating Arrangement**: 237 (37.1%)\n",
    "- **Split**: 574 train / 64 validation\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Expected Time\n",
    "- Data Preparation: ~2 minutes\n",
    "- Model 1 Training: ~15-30 minutes\n",
    "- Model 2 Training: ~15-30 minutes\n",
    "- **Total**: ~35-65 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Setup & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install torch transformers datasets trl peft unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt, train_on_responses_only\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "print(\"All packages imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CURATED_DIR = Path(\"/Users/777bhavyagoyal/Developer/UNSLOTHxAMDxHACk/MAIN_CURATED_JSON\")\n",
    "OUTPUT_DIR = Path(\"/Users/777bhavyagoyal/Developer/UNSLOTHxAMDxHACk/training_data\")\n",
    "MODELS_DIR = Path(\"/Users/777bhavyagoyal/Developer/UNSLOTHxAMDxHACk/models\")\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Directories created\")\n",
    "print(f\"Curated data: {CURATED_DIR}\")\n",
    "print(f\"Training data: {OUTPUT_DIR}\")\n",
    "print(f\"Models output: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 1: Load and Analyze Curated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all curated questions\n",
    "all_questions = []\n",
    "curated_files = sorted(CURATED_DIR.glob(\"*.json\"))\n",
    "\n",
    "for file_path in curated_files:\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            questions = json.load(f)\n",
    "            # Filter valid questions\n",
    "            for q in questions:\n",
    "                if len(q.get('choices', [])) == 4 and q.get('answer', '') in ['A', 'B', 'C', 'D']:\n",
    "                    all_questions.append(q)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path.name}: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Loaded {len(all_questions)} valid questions from {len(curated_files)} files\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nüìù Sample question:\")\n",
    "print(json.dumps(all_questions[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Prepare Question Generation Data\n",
    "\n",
    "**Model 1 Task**: Generate logical reasoning questions from scratch\n",
    "\n",
    "**Input**: `\"Generate a medium difficulty blood_relations question.\"`\n",
    "\n",
    "**Output**: Full question JSON with choices, answer, reasoning, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for Question Generation\n",
    "QUESTION_GEN_SYSTEM_PROMPT = \"\"\"You are an expert at creating high-quality logical reasoning questions for competitive exams and aptitude tests.\n",
    "\n",
    "You specialize in two types of questions:\n",
    "\n",
    "**1. Blood Relations**\n",
    "- Family relationship puzzles involving complex kinship chains\n",
    "- Include relationships like: father, mother, son, daughter, uncle, aunt, cousin, grandfather, grandmother, brother-in-law, sister-in-law\n",
    "- Questions should be self-contained with all necessary information stated explicitly\n",
    "- Example: \"A is the father of B. C is the mother of B. D is C's sister. E is D's husband. How is E related to B?\"\n",
    "\n",
    "**2. Seating Arrangement**\n",
    "- Spatial reasoning puzzles with people sitting in various configurations\n",
    "- Types: linear rows, circular arrangements, parallel rows, square tables\n",
    "- Include direction (facing north/south/center), positions (left/right, immediate/second/third), and constraints\n",
    "- Questions should provide clear spatial relationships and ask about deducible positions\n",
    "\n",
    "**Quality Requirements:**\n",
    "‚úì Self-contained: Include all facts needed to solve the question in the question itself\n",
    "‚úì Clear and unambiguous: No vague or confusing statements\n",
    "‚úì Exactly 4 choices: Always provide options A, B, C, D\n",
    "‚úì Unique correct answer: Only one option should be definitively correct\n",
    "‚úì Step-by-step reasoning: Provide 5 clear logical steps showing how to reach the answer\n",
    "‚úì Concise explanation: Brief summary of why the answer is correct\n",
    "‚úì Appropriate difficulty: Match the requested difficulty level (easy/medium/hard)\n",
    "\n",
    "**Output Format:**\n",
    "Return a valid JSON object with these exact fields:\n",
    "{\n",
    "  \"topic\": \"blood_relations\" or \"seating_arrangement\",\n",
    "  \"question\": \"<the question text with all context>\",\n",
    "  \"choices\": [\"A) <option>\", \"B) <option>\", \"C) <option>\", \"D) <option>\"],\n",
    "  \"answer\": \"A\" or \"B\" or \"C\" or \"D\",\n",
    "  \"explanation\": \"<brief explanation of the answer>\",\n",
    "  \"reasoning\": \"Step 1: ... Step 2: ... Step 3: ... Step 4: ... Step 5: ...\",\n",
    "  \"difficulty\": \"easy\" or \"medium\" or \"hard\"\n",
    "}\n",
    "\n",
    "Generate questions that would challenge students preparing for competitive exams while being solvable through logical reasoning.\"\"\"\n",
    "\n",
    "print(\"‚úÖ Question Generation System Prompt Created\")\n",
    "print(f\"\\nPrompt length: {len(QUESTION_GEN_SYSTEM_PROMPT)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Question Generation examples\n",
    "question_gen_examples = []\n",
    "\n",
    "for q in all_questions:\n",
    "    topic = q.get('topic', 'blood_relations')\n",
    "    difficulty = q.get('difficulty', 'medium')\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": QUESTION_GEN_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Generate a {difficulty} difficulty {topic.replace('_', ' ')} question with 4 multiple choice options, correct answer, explanation, and step-by-step reasoning.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": json.dumps(q, indent=2, ensure_ascii=False)\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    question_gen_examples.append({\"conversations\": conversation})\n",
    "\n",
    "print(f\"‚úÖ Created {len(question_gen_examples)} question generation examples\")\n",
    "\n",
    "# Shuffle and split\n",
    "random.seed(42)\n",
    "random.shuffle(question_gen_examples)\n",
    "split_idx = int(len(question_gen_examples) * 0.9)\n",
    "\n",
    "qgen_train = question_gen_examples[:split_idx]\n",
    "qgen_val = question_gen_examples[split_idx:]\n",
    "\n",
    "print(f\"\\nSplit:\")\n",
    "print(f\"  Train: {len(qgen_train)}\")\n",
    "print(f\"  Val: {len(qgen_val)}\")\n",
    "\n",
    "# Save\n",
    "with open(OUTPUT_DIR / \"question_gen_train.json\", 'w') as f:\n",
    "    json.dump(qgen_train, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(OUTPUT_DIR / \"question_gen_val.json\", 'w') as f:\n",
    "    json.dump(qgen_val, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved question generation data to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Prepare Answer Generation Data\n",
    "\n",
    "**Model 2 Task**: Solve logical reasoning questions with step-by-step reasoning\n",
    "\n",
    "**Input**: Question text + choices\n",
    "\n",
    "**Output**: Detailed reasoning + explanation + answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for Answer Generation\n",
    "ANSWER_GEN_SYSTEM_PROMPT = \"\"\"You are an expert at solving logical reasoning questions using systematic step-by-step analysis.\n",
    "\n",
    "You excel at two types of logical reasoning:\n",
    "\n",
    "**1. Blood Relations**\n",
    "- Carefully track each family relationship mentioned\n",
    "- Build a mental family tree to visualize connections\n",
    "- Identify the relationship chain from person A to person B\n",
    "- Consider both direct relationships and relationships through marriage\n",
    "- Common relationships: parent, child, sibling, uncle/aunt, cousin, in-law, grandparent\n",
    "\n",
    "**2. Seating Arrangement**\n",
    "- Note the arrangement type (linear row, circle, parallel rows, etc.)\n",
    "- Track explicit position information (\"sits at position 3\", \"at extreme end\", etc.)\n",
    "- Track relative positions (\"left of\", \"right of\", \"opposite to\", \"between\", etc.)\n",
    "- Consider facing direction when specified\n",
    "- Use process of elimination to deduce unknown positions\n",
    "- Verify final arrangement satisfies all given constraints\n",
    "\n",
    "**Solution Approach:**\n",
    "\n",
    "1. **Read Carefully**: Identify all given facts and relationships\n",
    "2. **Organize Information**: Create a mental model (family tree or seating diagram)\n",
    "3. **Apply Logic**: Use deduction to find new relationships or positions\n",
    "4. **Verify**: Check that your answer satisfies all given constraints\n",
    "5. **Explain**: Provide clear step-by-step reasoning\n",
    "\n",
    "**Output Format:**\n",
    "\n",
    "**Reasoning:**\n",
    "Step 1: [First logical step]\n",
    "Step 2: [Second logical step]\n",
    "Step 3: [Third logical step]\n",
    "Step 4: [Fourth logical step]\n",
    "Step 5: [Conclusion]\n",
    "\n",
    "**Explanation:**\n",
    "[Brief summary of why this answer is correct]\n",
    "\n",
    "**Answer:** [A/B/C/D]\n",
    "\n",
    "Always provide clear, logical reasoning that anyone can follow to understand how you arrived at the correct answer.\"\"\"\n",
    "\n",
    "print(\"‚úÖ Answer Generation System Prompt Created\")\n",
    "print(f\"\\nPrompt length: {len(ANSWER_GEN_SYSTEM_PROMPT)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for answer generation format\n",
    "def format_question_with_choices(question, choices):\n",
    "    \"\"\"Format question with choices for display\"\"\"\n",
    "    formatted = f\"{question}\\n\\nChoices:\\n\"\n",
    "    for choice in choices:\n",
    "        formatted += f\"{choice}\\n\"\n",
    "    return formatted.strip()\n",
    "\n",
    "def format_answer(answer, reasoning, explanation):\n",
    "    \"\"\"Format the answer with reasoning and explanation\"\"\"\n",
    "    response = f\"**Reasoning:**\\n{reasoning}\\n\\n\"\n",
    "    response += f\"**Explanation:**\\n{explanation}\\n\\n\"\n",
    "    response += f\"**Answer:** {answer}\"\n",
    "    return response\n",
    "\n",
    "# Create Answer Generation examples\n",
    "answer_gen_examples = []\n",
    "\n",
    "for q in all_questions:\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": ANSWER_GEN_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": format_question_with_choices(\n",
    "                q.get('question', ''),\n",
    "                q.get('choices', [])\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": format_answer(\n",
    "                q.get('answer', ''),\n",
    "                q.get('reasoning', ''),\n",
    "                q.get('explanation', '')\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    answer_gen_examples.append({\"conversations\": conversation})\n",
    "\n",
    "print(f\"‚úÖ Created {len(answer_gen_examples)} answer generation examples\")\n",
    "\n",
    "# Shuffle and split\n",
    "random.seed(42)\n",
    "random.shuffle(answer_gen_examples)\n",
    "split_idx = int(len(answer_gen_examples) * 0.9)\n",
    "\n",
    "agen_train = answer_gen_examples[:split_idx]\n",
    "agen_val = answer_gen_examples[split_idx:]\n",
    "\n",
    "print(f\"\\nSplit:\")\n",
    "print(f\"  Train: {len(agen_train)}\")\n",
    "print(f\"  Val: {len(agen_val)}\")\n",
    "\n",
    "# Save\n",
    "with open(OUTPUT_DIR / \"answer_gen_train.json\", 'w') as f:\n",
    "    json.dump(agen_train, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(OUTPUT_DIR / \"answer_gen_val.json\", 'w') as f:\n",
    "    json.dump(agen_val, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved answer generation data to {OUTPUT_DIR}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìù Sample Answer Generation Example:\")\n",
    "print(json.dumps(agen_train[0]['conversations'], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Train Question Generation Model\n",
    "\n",
    "**Model**: Llama-3.2-3B-Instruct  \n",
    "**Task**: Generate logical reasoning questions  \n",
    "**Training Time**: ~15-30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Question Generation Model\n",
    "QGEN_CONFIG = {\n",
    "    \"model_name\": \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    \"max_seq_length\": 2048,  # Longer for full question JSON\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_accumulation\": 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"warmup_steps\": 10,\n",
    "}\n",
    "\n",
    "print(\"üìã Question Generation Model Configuration:\")\n",
    "for key, value in QGEN_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "print(\"\\nüì• Loading Question Generation base model...\")\n",
    "\n",
    "qgen_model, qgen_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=QGEN_CONFIG[\"model_name\"],\n",
    "    max_seq_length=QGEN_CONFIG[\"max_seq_length\"],\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=False,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded {QGEN_CONFIG['model_name']}\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "print(\"\\nüîß Adding LoRA adapters...\")\n",
    "\n",
    "qgen_model = FastLanguageModel.get_peft_model(\n",
    "    qgen_model,\n",
    "    r=QGEN_CONFIG[\"lora_r\"],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=QGEN_CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LoRA adapters added (r={QGEN_CONFIG['lora_r']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "print(\"\\nüìä Loading question generation datasets...\")\n",
    "\n",
    "qgen_train_dataset = Dataset.from_list(qgen_train)\n",
    "qgen_val_dataset = Dataset.from_list(qgen_val)\n",
    "\n",
    "print(f\"Train: {len(qgen_train_dataset)} examples\")\n",
    "print(f\"Val: {len(qgen_val_dataset)} examples\")\n",
    "\n",
    "# Set chat template\n",
    "qgen_tokenizer = get_chat_template(qgen_tokenizer, chat_template=\"llama-3.1\")\n",
    "\n",
    "if qgen_tokenizer.pad_token is None:\n",
    "    qgen_tokenizer.pad_token = qgen_tokenizer.eos_token\n",
    "    qgen_tokenizer.pad_token_id = qgen_tokenizer.eos_token_id\n",
    "\n",
    "# Formatting function\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    for convo in convos:\n",
    "        if isinstance(convo, list):\n",
    "            text = qgen_tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "print(\"\\nüîß Formatting datasets...\")\n",
    "\n",
    "qgen_train_dataset = standardize_sharegpt(qgen_train_dataset)\n",
    "qgen_train_dataset = qgen_train_dataset.map(formatting_prompts_func, batched=True, remove_columns=qgen_train_dataset.column_names)\n",
    "qgen_train_dataset = qgen_train_dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "qgen_val_dataset = standardize_sharegpt(qgen_val_dataset)\n",
    "qgen_val_dataset = qgen_val_dataset.map(formatting_prompts_func, batched=True, remove_columns=qgen_val_dataset.column_names)\n",
    "qgen_val_dataset = qgen_val_dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "print(f\"‚úÖ Formatted {len(qgen_train_dataset)} train + {len(qgen_val_dataset)} val examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "print(\"\\nüöÄ Setting up Question Generation trainer...\")\n",
    "\n",
    "qgen_output_dir = MODELS_DIR / \"question_gen_model\"\n",
    "qgen_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "qgen_trainer = SFTTrainer(\n",
    "    model=qgen_model,\n",
    "    tokenizer=qgen_tokenizer,\n",
    "    train_dataset=qgen_train_dataset,\n",
    "    eval_dataset=qgen_val_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=QGEN_CONFIG[\"max_seq_length\"],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=qgen_tokenizer, padding=True),\n",
    "    packing=False,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=QGEN_CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=QGEN_CONFIG[\"batch_size\"],\n",
    "        gradient_accumulation_steps=QGEN_CONFIG[\"gradient_accumulation\"],\n",
    "        warmup_steps=QGEN_CONFIG[\"warmup_steps\"],\n",
    "        num_train_epochs=QGEN_CONFIG[\"num_epochs\"],\n",
    "        learning_rate=QGEN_CONFIG[\"learning_rate\"],\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=str(qgen_output_dir / \"checkpoints\"),\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Train only on responses\n",
    "qgen_trainer = train_on_responses_only(\n",
    "    qgen_trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Question Generation Model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèãÔ∏è  TRAINING QUESTION GENERATION MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis will take ~15-30 minutes. Go grab a coffee! ‚òï\\n\")\n",
    "\n",
    "FastLanguageModel.for_training(qgen_model)\n",
    "qgen_stats = qgen_trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ QUESTION GENERATION TRAINING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Question Generation Model\n",
    "print(\"\\nüíæ Saving Question Generation model...\")\n",
    "\n",
    "qgen_lora_path = qgen_output_dir / \"lora\"\n",
    "qgen_merged_path = qgen_output_dir / \"merged\"\n",
    "\n",
    "qgen_lora_path.mkdir(exist_ok=True)\n",
    "qgen_merged_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Save LoRA adapters\n",
    "qgen_model.save_pretrained(str(qgen_lora_path))\n",
    "qgen_tokenizer.save_pretrained(str(qgen_lora_path))\n",
    "print(f\"‚úÖ LoRA adapters saved to: {qgen_lora_path}\")\n",
    "\n",
    "# Save merged model\n",
    "qgen_model.save_pretrained_merged(str(qgen_merged_path), qgen_tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"‚úÖ Merged model saved to: {qgen_merged_path}\")\n",
    "\n",
    "print(\"\\nüéâ Question Generation Model Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Train Answer Generation Model\n",
    "\n",
    "**Model**: Llama-3.2-3B-Instruct  \n",
    "**Task**: Solve logical reasoning questions  \n",
    "**Training Time**: ~15-30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory from Question Gen model\n",
    "import gc\n",
    "del qgen_model, qgen_tokenizer, qgen_trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ Memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Answer Generation Model\n",
    "AGEN_CONFIG = {\n",
    "    \"model_name\": \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    \"max_seq_length\": 1536,  # Moderate for Q&A\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"batch_size\": 8,\n",
    "    \"gradient_accumulation\": 2,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"warmup_steps\": 10,\n",
    "}\n",
    "\n",
    "print(\"üìã Answer Generation Model Configuration:\")\n",
    "for key, value in AGEN_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "print(\"\\nüì• Loading Answer Generation base model...\")\n",
    "\n",
    "agen_model, agen_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=AGEN_CONFIG[\"model_name\"],\n",
    "    max_seq_length=AGEN_CONFIG[\"max_seq_length\"],\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=False,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded {AGEN_CONFIG['model_name']}\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "print(\"\\nüîß Adding LoRA adapters...\")\n",
    "\n",
    "agen_model = FastLanguageModel.get_peft_model(\n",
    "    agen_model,\n",
    "    r=AGEN_CONFIG[\"lora_r\"],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=AGEN_CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LoRA adapters added (r={AGEN_CONFIG['lora_r']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "print(\"\\nüìä Loading answer generation datasets...\")\n",
    "\n",
    "agen_train_dataset = Dataset.from_list(agen_train)\n",
    "agen_val_dataset = Dataset.from_list(agen_val)\n",
    "\n",
    "print(f\"Train: {len(agen_train_dataset)} examples\")\n",
    "print(f\"Val: {len(agen_val_dataset)} examples\")\n",
    "\n",
    "# Set chat template\n",
    "agen_tokenizer = get_chat_template(agen_tokenizer, chat_template=\"llama-3.1\")\n",
    "\n",
    "if agen_tokenizer.pad_token is None:\n",
    "    agen_tokenizer.pad_token = agen_tokenizer.eos_token\n",
    "    agen_tokenizer.pad_token_id = agen_tokenizer.eos_token_id\n",
    "\n",
    "# Formatting function\n",
    "def formatting_prompts_func_agen(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    for convo in convos:\n",
    "        if isinstance(convo, list):\n",
    "            text = agen_tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "print(\"\\nüîß Formatting datasets...\")\n",
    "\n",
    "agen_train_dataset = standardize_sharegpt(agen_train_dataset)\n",
    "agen_train_dataset = agen_train_dataset.map(formatting_prompts_func_agen, batched=True, remove_columns=agen_train_dataset.column_names)\n",
    "agen_train_dataset = agen_train_dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "agen_val_dataset = standardize_sharegpt(agen_val_dataset)\n",
    "agen_val_dataset = agen_val_dataset.map(formatting_prompts_func_agen, batched=True, remove_columns=agen_val_dataset.column_names)\n",
    "agen_val_dataset = agen_val_dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "print(f\"‚úÖ Formatted {len(agen_train_dataset)} train + {len(agen_val_dataset)} val examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "print(\"\\nüöÄ Setting up Answer Generation trainer...\")\n",
    "\n",
    "agen_output_dir = MODELS_DIR / \"answer_gen_model\"\n",
    "agen_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "agen_trainer = SFTTrainer(\n",
    "    model=agen_model,\n",
    "    tokenizer=agen_tokenizer,\n",
    "    train_dataset=agen_train_dataset,\n",
    "    eval_dataset=agen_val_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=AGEN_CONFIG[\"max_seq_length\"],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=agen_tokenizer, padding=True),\n",
    "    packing=False,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=AGEN_CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=AGEN_CONFIG[\"batch_size\"],\n",
    "        gradient_accumulation_steps=AGEN_CONFIG[\"gradient_accumulation\"],\n",
    "        warmup_steps=AGEN_CONFIG[\"warmup_steps\"],\n",
    "        num_train_epochs=AGEN_CONFIG[\"num_epochs\"],\n",
    "        learning_rate=AGEN_CONFIG[\"learning_rate\"],\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=str(agen_output_dir / \"checkpoints\"),\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Train only on responses\n",
    "agen_trainer = train_on_responses_only(\n",
    "    agen_trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Answer Generation Model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèãÔ∏è  TRAINING ANSWER GENERATION MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis will take ~15-30 minutes. Another coffee? ‚òï\\n\")\n",
    "\n",
    "FastLanguageModel.for_training(agen_model)\n",
    "agen_stats = agen_trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ANSWER GENERATION TRAINING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Answer Generation Model\n",
    "print(\"\\nüíæ Saving Answer Generation model...\")\n",
    "\n",
    "agen_lora_path = agen_output_dir / \"lora\"\n",
    "agen_merged_path = agen_output_dir / \"merged\"\n",
    "\n",
    "agen_lora_path.mkdir(exist_ok=True)\n",
    "agen_merged_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Save LoRA adapters\n",
    "agen_model.save_pretrained(str(agen_lora_path))\n",
    "agen_tokenizer.save_pretrained(str(agen_lora_path))\n",
    "print(f\"‚úÖ LoRA adapters saved to: {agen_lora_path}\")\n",
    "\n",
    "# Save merged model\n",
    "agen_model.save_pretrained_merged(str(agen_merged_path), agen_tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"‚úÖ Merged model saved to: {agen_merged_path}\")\n",
    "\n",
    "print(\"\\nüéâ Answer Generation Model Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéâ Training Complete!\n",
    "\n",
    "## Your Models\n",
    "\n",
    "### Question Generation Model\n",
    "- **LoRA**: `models/question_gen_model/lora/`\n",
    "- **Merged**: `models/question_gen_model/merged/` ‚≠ê\n",
    "\n",
    "### Answer Generation Model\n",
    "- **LoRA**: `models/answer_gen_model/lora/`\n",
    "- **Merged**: `models/answer_gen_model/merged/` ‚≠ê\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. ‚úÖ Test your models (see inference notebook)\n",
    "2. ‚úÖ Deploy for your hackathon project\n",
    "3. ‚úÖ Generate new questions and solve them!\n",
    "\n",
    "## System Prompts\n",
    "\n",
    "The optimized system prompts used for training are saved above. Use these same prompts during inference for best results!\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéä You now have two specialized models for logical reasoning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
